---
title: 'Blog Post number 0'
date: 2019-08-10
permalink: /posts/2019/08/blog-post-0/
tags:
  - cool posts
  - category1
  - category2
---

======
# Hadoop 单节点配置

## 配置Hadoop之前；
下载解压的目录在home/opt/hadoop/latest/下面这样方便管理也方便做版本控制

> mkdir -p /home/opt/hadoop/latest/

> tar -zxvf hadoop-xxx.tar.gz -C /home/opt/hadoop/latest/

1. linux系统安装JDK

先运行 Java -version
看一下是不是1.8系列的,是就vans了，没什么好说的，这个版本的java我最喜欢
如果没有Java则需要重新装java了哈。


2. SSH免密

因为hadoop为分布式框架，涉及多节点协调工作，配置这个更为方便，不配置的话，就算单节点运行也会让你总是输入密码，不配麻烦点也能跑

> $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

> $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

实验一下，ssh yourhost 看是否不需要输入密码，如果成功免密配置成功，失败的话可以找找教程看看有啥解决方法。

----

ps：解压hadoop之后则需要进行环境变量的配置
hadoop 就像一个独立的文件系统，目录设置的很科学，配置文件都落在etc/hadoop里面

----

## 一、配置Hadoop

1. Hadoop 同 jdk绑定

首先这个系统是Java 编写的，所以运行环境基于JVM,理所应当需要同系统JAVA环境进行绑定 vim conf/hadoop-env.sh文件
添加export JAVA_HOME=jdk的路径
如果不记得可以$echo JAVA_HOME 得到以上jdk的路径

2. 核心配置文件etc/hadoop/core-site.xml:这个是配置hdfs的主要配置（访问方式）这里主要是将以下localhost的配置,在配置之前需要

  > cat /etc/hosts

 看一下系统的hosts 文件是否有域名同ip的映射，也就是说以下localhost可以是任何有意义的字串，只要hosts有字串同ip的映射
 另外则是对hadoop/tmp的配置
LINUX 系统，在服务重启后，如果不对hadoop的tmp进行配置，默认的tmp是存在一个重启后会被清空的目录hadoop.tmp.dir 是 hadoop文件系统依赖的基本配置，很多配置路径都依赖它，它的默认位置在/tmp/{$user}下面。
所以建议在core-site.xml中配置 hadoop.tmp.dir 路径到持久化目录，可以配置到一个不需要重启后重新挂载的目录，因为不会有很大的数据落在这上面所以建议可以设为/home/opt/hadoop/latest/hadoopxx/tmp。

```
<configuration>

     <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
     </property>

     <property> //指定tmp目录的位置 tmp目录提前创建好
        <name>hadoop.tmp.dir</name>
        <value>/home/opt/hadoop/latest/hadoopxx/tmp</value>
     </property>

</configuration>
```

3. etc/hadoop/hdfs-site.xml:这是配置hdfs涉及文件系统的配置文件，这里可以设置hdfs在建立文件时产生副本的参数

```
<configuration>

     <property>

        <name>dfs.replication</name>

         <value>1</value>

     </property>

</configuration>
```

4. etc/hadoop/mapred-site.xml参数如下，这一步是配置MR（map-reduce工作模式）基于Yarn框架运行：

```
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

既然配置了Yarn就要配全套，配置etc/hadoop/yarn-site.xml参数如下：
```
<configuration>
<!--执行MR时候经过shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
<!--指定resourcemanager的主机名 -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>[hostname]</value>
    </property>
</configuration>
```
5. 设置hdfs的默认数据存储路径

分别设置dfs相关目录，在hdfs-site.xml
```
　　<!---name目录-->

　　　　<property>

　　　　　　<name>dfs.namenode.name.dir</name>

　　　　　　<value>file://your_dir/dfs/name</value>
　　　　　　<description>
　　　　　　　　确定DFS名称节点应在本地文件系统的哪个位置存储名称表（fsimage）。
       如果这是一个以逗号分隔的目录列表，则名称表将被复制到所有目录中，以实现冗余
　　　　　　 </description>
　　　　</property>

　　<!---data目录--->
    <!---确定DFS数据节点应该在本地文件系统上存储块的位置。
  如果这是以逗号分隔的目录列表，则数据将存储在所有已命名的目录中，通常位于不同的设备上。 应该为HDFS存储策略标记相应的存储类型（[SSD] / [磁盘] / [存档] / [RAM_DISK]）。
如果目录没有显式标记存储类型，则默认存储类型为DISK。 如果本地文件系统权限允许，则不存在的目录将被创建。--->
　　　　<property>
　　　　　　<name>dfs.datanode.data.dir</name>
　　　　　　<value>file://your_dir/dfs/data</value>
　　　　</property>
　  <!---namesecondary目录,DFS辅助名称节点应该存储要合并的临时图像的位置-->

　　　　<property>
　　　　　　<name>dfs.namenode.checkpoint.dir</name>
　　　　　　<value>file://your_dir/dfs/namesecondary</value>
　　　　</property>
```

## 二、startup

1、格式化hadoop的文件系统HDFS(建立文件目录，和清除数据)有一个坑是千万不要再启动之后再格式化，这样后面看到datanode没能启动。

> $ bin/hadoop namenode -format

2、运行hadoop

> $ sbin/start-all.sh

3、检验是否启动成功

NameNode - http://localhost:50070/

JobTracker - http://localhost:50030/

4、退出hadoop

$ sbin/stop-all.sh

检验单节点是否成功启动hadoop方法一般为运行jps指令看是否有NameNode,DataNode进程存在， http://localhost:50070/ 页面可以访问

注意，如果Hadoop 启动 namenode datanode die 了，修改你的上述配置中的hostname为localhost 或者127.0.0.1，重启Hadoop
======


------